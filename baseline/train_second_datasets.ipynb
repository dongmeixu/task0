{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/search/odin/yangyuran/program/Anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/search/odin/yangyuran/program/Anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# sys.path.append(\"/search/odin/yangyuran/program/Anaconda3/envs/tensorflow/lib/python3.6/site-packages/\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import data_process\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 指定参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "N_CLASSES = 2\n",
    "IMG_W = 256  # resize the image, if the input image is too large, training will be very slow.\n",
    "IMG_H = 256\n",
    "IMG_C = 3\n",
    "RATIO = 0.2  # take 20% of dataset as validation data\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 100 + 3 * BATCH_SIZE\n",
    "MAX_STEP = 5000  # with current parameters, it is suggested to use MAX_STEP>10k\n",
    "learning_rate = 0.0001  # with current parameters, it is suggested to use learning rate<0.0001\n",
    "\n",
    "train_dir = '/search/odin/xudongmei/working/datasets/bak2_crop/train_split/'\n",
    "test_dir = '/search/odin/xudongmei/working/datasets/bak2_crop/test_split/'\n",
    "logs_train_dir = './logs/bak2_crop/train/batch_size=64/lr=0.0001/'\n",
    "logs_val_dir = './logs/bak2_crop/val/batch_size=64/lr=0.0001/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    with tf.Graph().as_default():\n",
    "        train, train_label, val, val_label = data_process.get_files(train_dir, RATIO)\n",
    "\n",
    "        train_batch, train_label_batch = data_process.get_batch(train,\n",
    "                                                                train_label,\n",
    "                                                                IMG_W,\n",
    "                                                                IMG_H,\n",
    "                                                                BATCH_SIZE,\n",
    "                                                                CAPACITY)\n",
    "        val_batch, val_label_batch = data_process.get_batch(val,\n",
    "                                                            val_label,\n",
    "                                                            IMG_W,\n",
    "                                                            IMG_H,\n",
    "                                                            BATCH_SIZE,\n",
    "                                                            CAPACITY)\n",
    "\n",
    "        # 输入数据的命名空间\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, IMG_C], name=\"X_input\")\n",
    "            y_ = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=\"Y_input\")\n",
    "            keep_prob = tf.placeholder(tf.float32, name='Keep_prob')\n",
    "\n",
    "        logits = model.inference(x, BATCH_SIZE, N_CLASSES)\n",
    "        loss = model.losses(logits, y_)\n",
    "        acc = model.evaluation(logits, y_)\n",
    "        train_op = model.trainning(loss, learning_rate)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(logs_val_dir)\n",
    "\n",
    "            try:\n",
    "                for step in np.arange(MAX_STEP):\n",
    "                    if coord.should_stop():\n",
    "                        break\n",
    "\n",
    "                    tra_images, tra_labels = sess.run([train_batch, train_label_batch])  # 每个epoch(step)读取的数据顺序不是一致的\n",
    "                    #                     print(\"每次数据都是一样的嘛？\")\n",
    "                    #                     print(tra_labels[:5])\n",
    "\n",
    "                    _, tra_loss, tra_acc = sess.run([train_op, loss, acc],\n",
    "                                                    feed_dict={x: tra_images, y_: tra_labels, keep_prob: 0.5})\n",
    "                    if step % 50 == 0:\n",
    "                        print('Step %d, train loss = %.2f, train accuracy = %.2f%%' % (step, tra_loss, tra_acc * 100.0))\n",
    "                        summary_str = sess.run(summary_op, feed_dict={x: tra_images, y_: tra_labels, keep_prob: 0.5})\n",
    "                        train_writer.add_summary(summary_str, step)\n",
    "\n",
    "                    if step % 100 == 0 or (step + 1) == MAX_STEP:\n",
    "                        val_images, val_labels = sess.run([val_batch, val_label_batch])\n",
    "                        val_loss, val_acc = sess.run([loss, acc],\n",
    "                                                     feed_dict={x: val_images, y_: val_labels, keep_prob: 0.5})\n",
    "                        print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' % (\n",
    "                        step, val_loss, val_acc * 100.0))\n",
    "                        summary_str = sess.run(summary_op, feed_dict={x: val_images, y_: val_labels, keep_prob: 0.5})\n",
    "                        val_writer.add_summary(summary_str, step)\n",
    "\n",
    "                    if step % 1000 == 0 or (step + 1) == MAX_STEP:\n",
    "                        checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "                        saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Done training -- epoch limit reached')\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "            coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 baidu\n",
      "There are 2880 sogou\n",
      "Step 0, train loss = 0.69, train accuracy = 43.75%\n",
      "**  Step 0, val loss = 0.69, val accuracy = 53.12%  **\n",
      "Step 50, train loss = 0.65, train accuracy = 65.62%\n",
      "Step 100, train loss = 0.48, train accuracy = 76.56%\n",
      "Step 150, train loss = 0.27, train accuracy = 87.50%\n",
      "Step 200, train loss = 0.20, train accuracy = 92.19%\n",
      "**  Step 200, val loss = 0.28, val accuracy = 89.06%  **\n",
      "Step 250, train loss = 0.18, train accuracy = 93.75%\n",
      "Step 300, train loss = 0.14, train accuracy = 95.31%\n",
      "Step 350, train loss = 0.10, train accuracy = 98.44%\n",
      "Step 400, train loss = 0.16, train accuracy = 93.75%\n",
      "**  Step 400, val loss = 0.10, val accuracy = 98.44%  **\n",
      "Step 450, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 500, train loss = 0.05, train accuracy = 98.44%\n",
      "Step 550, train loss = 0.14, train accuracy = 93.75%\n",
      "Step 600, train loss = 0.05, train accuracy = 96.88%\n",
      "**  Step 600, val loss = 0.16, val accuracy = 98.44%  **\n",
      "Step 650, train loss = 0.20, train accuracy = 96.88%\n",
      "Step 700, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 750, train loss = 0.03, train accuracy = 96.88%\n",
      "Step 800, train loss = 0.04, train accuracy = 96.88%\n",
      "**  Step 800, val loss = 0.07, val accuracy = 96.88%  **\n",
      "Step 850, train loss = 0.04, train accuracy = 96.88%\n",
      "Step 900, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 950, train loss = 0.02, train accuracy = 98.44%\n",
      "Step 1000, train loss = 0.01, train accuracy = 98.44%\n",
      "**  Step 1000, val loss = 0.07, val accuracy = 96.88%  **\n",
      "Step 1050, train loss = 0.02, train accuracy = 98.44%\n",
      "Step 1100, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1150, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 1200, train loss = 0.03, train accuracy = 98.44%\n",
      "**  Step 1200, val loss = 0.06, val accuracy = 98.44%  **\n",
      "Step 1250, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 1300, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1350, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1400, train loss = 0.02, train accuracy = 98.44%\n",
      "**  Step 1400, val loss = 0.33, val accuracy = 95.31%  **\n",
      "Step 1450, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 1500, train loss = 0.01, train accuracy = 98.44%\n",
      "Step 1550, train loss = 0.01, train accuracy = 98.44%\n",
      "Step 1600, train loss = 0.01, train accuracy = 98.44%\n",
      "**  Step 1600, val loss = 0.04, val accuracy = 98.44%  **\n",
      "Step 1650, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 1700, train loss = 0.07, train accuracy = 98.44%\n",
      "Step 1750, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1800, train loss = 0.06, train accuracy = 96.88%\n",
      "**  Step 1800, val loss = 0.24, val accuracy = 96.88%  **\n",
      "Step 1850, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1900, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 1950, train loss = 0.01, train accuracy = 98.44%\n",
      "Step 2000, train loss = 0.01, train accuracy = 100.00%\n",
      "**  Step 2000, val loss = 0.01, val accuracy = 100.00%  **\n",
      "Step 2050, train loss = 0.03, train accuracy = 98.44%\n",
      "Step 2100, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2150, train loss = 0.03, train accuracy = 98.44%\n",
      "Step 2200, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 2200, val loss = 0.04, val accuracy = 98.44%  **\n",
      "Step 2250, train loss = 0.02, train accuracy = 98.44%\n",
      "Step 2300, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 2350, train loss = 0.02, train accuracy = 98.44%\n",
      "Step 2400, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 2400, val loss = 0.25, val accuracy = 95.31%  **\n",
      "Step 2450, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2500, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2550, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2600, train loss = 0.03, train accuracy = 98.44%\n",
      "**  Step 2600, val loss = 0.06, val accuracy = 96.88%  **\n",
      "Step 2650, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2700, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 2750, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 2800, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 2800, val loss = 0.09, val accuracy = 98.44%  **\n",
      "Step 2850, train loss = 0.05, train accuracy = 98.44%\n",
      "Step 2900, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 2950, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 3000, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 3000, val loss = 0.03, val accuracy = 98.44%  **\n",
      "Step 3050, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 3100, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3150, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3200, train loss = 0.01, train accuracy = 100.00%\n",
      "**  Step 3200, val loss = 0.09, val accuracy = 98.44%  **\n",
      "Step 3250, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3300, train loss = 0.04, train accuracy = 96.88%\n",
      "Step 3350, train loss = 0.01, train accuracy = 98.44%\n",
      "Step 3400, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 3400, val loss = 0.38, val accuracy = 96.88%  **\n",
      "Step 3450, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3500, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 3550, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3600, train loss = 0.01, train accuracy = 100.00%\n",
      "**  Step 3600, val loss = 0.21, val accuracy = 98.44%  **\n",
      "Step 3650, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 3700, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3750, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3800, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 3800, val loss = 0.17, val accuracy = 95.31%  **\n",
      "Step 3850, train loss = 0.19, train accuracy = 98.44%\n",
      "Step 3900, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 3950, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4000, train loss = 0.01, train accuracy = 100.00%\n",
      "**  Step 4000, val loss = 0.00, val accuracy = 100.00%  **\n",
      "Step 4050, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4100, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4150, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 4200, train loss = 0.01, train accuracy = 100.00%\n",
      "**  Step 4200, val loss = 0.00, val accuracy = 100.00%  **\n",
      "Step 4250, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4300, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4350, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4400, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 4400, val loss = 0.03, val accuracy = 98.44%  **\n",
      "Step 4450, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4500, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4550, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4600, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 4600, val loss = 0.00, val accuracy = 100.00%  **\n",
      "Step 4650, train loss = 0.03, train accuracy = 98.44%\n",
      "Step 4700, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4750, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 4800, train loss = 0.00, train accuracy = 100.00%\n",
      "**  Step 4800, val loss = 0.00, val accuracy = 100.00%  **\n",
      "Step 4850, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 4900, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 4950, train loss = 0.10, train accuracy = 98.44%\n",
      "**  Step 4999, val loss = 0.09, val accuracy = 98.44%  **\n",
      "Training time is:  0:27:23.475842\n"
     ]
    }
   ],
   "source": [
    "begin = datetime.datetime.now()\n",
    "run_training()\n",
    "end = datetime.datetime.now()\n",
    "print(\"Training time is: \", end - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    with tf.Graph().as_default():\n",
    "        test, test_label = data_process.get_files(test_dir, 0, False)\n",
    "        test_batch, test_label_batch = data_process.get_batch(test,\n",
    "                                                              test_label,\n",
    "                                                              IMG_W,\n",
    "                                                              IMG_H,\n",
    "                                                              BATCH_SIZE,\n",
    "                                                              CAPACITY)\n",
    "        print(test_batch.shape, test_label_batch.shape)  # (64, 256, 256, 3) (64,)\n",
    "        with tf.Session() as sess:\n",
    "            x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, IMG_C])\n",
    "            y_ = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            logits = model.inference(x, BATCH_SIZE, N_CLASSES)\n",
    "            logits = tf.nn.softmax(logits)\n",
    "            acc = model.evaluation(logits, y_)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            # 模型恢复\n",
    "            print(\"Reading checkpoints...\")\n",
    "            ckpt = tf.train.get_checkpoint_state(logs_train_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print('Loading success, global_step is %s' % global_step)\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "                return\n",
    "\n",
    "            # Start the queue runners.\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = []\n",
    "            try:\n",
    "                for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "                    threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                                     start=True))\n",
    "\n",
    "                num_iter = int(math.ceil(len(test) / BATCH_SIZE))\n",
    "                total_acc = 0  # Counts the total predictions.\n",
    "                total_sample_count = num_iter * BATCH_SIZE\n",
    "                step = 0\n",
    "                print(num_iter)\n",
    "\n",
    "                test_images, test_labels = sess.run([test_batch, test_label_batch])\n",
    "                # print(\"test_images dtype: \", test_images.dtype)  # test_images dtype:  float32\n",
    "                # print(\"test_images type: \", type(test_images))  # test_images type:  <class 'numpy.ndarray'>\n",
    "\n",
    "                while step < num_iter and not coord.should_stop():\n",
    "                    # 每次预测的准确率\n",
    "                    predictions, accuracies = sess.run([logits, acc],\n",
    "                                                       feed_dict={x: test_images, y_: test_labels, keep_prob: 1.0})\n",
    "                    # 计算总的准确率\n",
    "                    total_acc += np.sum(accuracies)\n",
    "                    # print(total_acc)\n",
    "                    step += 1\n",
    "\n",
    "                print(len(predictions))\n",
    "                # Compute precision @ 1.\n",
    "                avg_accuracies = total_acc / num_iter\n",
    "                print('%s: precision @ 1 = %.3f' % (datetime.datetime.now(), avg_accuracies))\n",
    "\n",
    "                \"\"\"将预测结果写入文件\"\"\"\n",
    "                print('Begin to write submission file ..')\n",
    "                f_submit = open('predict_2.csv', 'w')\n",
    "                f_submit.write('image_ids,baidu,sogou\\n')\n",
    "\n",
    "                print(len(test))\n",
    "\n",
    "                for i, image_name in enumerate(test[:BATCH_SIZE]):\n",
    "                    pred = ['%.2f' % p for p in predictions[i, :]]\n",
    "                    f_submit.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "\n",
    "                f_submit.close()\n",
    "\n",
    "                print('Submission file successfully generated!')\n",
    "\n",
    "            except Exception as e:  # pylint: disable=broad-except\n",
    "                coord.request_stop(e)\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "            coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 baidu\n",
      "There are 720 sogou\n",
      "(64, 256, 256, 3) (64,)\n",
      "Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./logs/bak2_crop/train/batch_size=64/lr=0.0001/model.ckpt-4999\n",
      "Loading success, global_step is 4999\n",
      "20\n",
      "64\n",
      "2018-05-24 18:24:50.314620: precision @ 1 = 0.960\n",
      "Begin to write submission file ..\n",
      "1220\n",
      "Submission file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
